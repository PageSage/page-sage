{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Page Sage\n",
    "\n",
    "This notebook should serve as a tutorial and explanatory process for the Page Sage recommendation algorithm.\n",
    "\n",
    "First, let's create some preprocessors for the data since we know what format to expect them.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class BookClassifier(object):\n",
    "    def __init__(self, volumes=[], ratings=[]):\n",
    "        if (len(volumes) == 0) or (len(ratings) == 0):\n",
    "            raise ValueError('Initial values cannot be zero')\n",
    "        if (len(volumes) != len(ratings)):\n",
    "            raise ValueError('Labels and data must have the same length')\n",
    "\n",
    "        self.volumes = volumes\n",
    "        self.ratings = ratings\n",
    "        self.__book_data = []\n",
    "        self.categories = dict()\n",
    "        self.maturities = dict()\n",
    "        self.books = []\n",
    "        self.X_train, self.y_train = [],[]\n",
    "        self.__model = None\n",
    "        self.set_up()\n",
    "\n",
    "\n",
    "    def set_up(self):\n",
    "        self.__book_data = self.__pull_books(self.volumes, self.ratings)\n",
    "        self.__find_categories(self.__book_data)\n",
    "        #self.maturities = self.__find_maturities(self.book_data)\n",
    "        self.books = self.__reformat(self.__book_data)\n",
    "        self.X_train, self.y_train = self.__x_y_train(self.books)\n",
    "        ##self.scikit_kpca = KernelPCA(n_components=3, kernel='rbf')\n",
    "        ##self.X_train = self.scikit_kpca.fit_transform(self.X_train)\n",
    "\n",
    "\n",
    "    def __pull_books(self, volumes, labels=[]):\n",
    "        book_data = []\n",
    "        search_key = str(os.environ.get('SEARCH_KEY'))\n",
    "        baseURL = 'https://www.googleapis.com/books/v1/volumes/'\n",
    "        endURL = '?key=' + search_key\n",
    "\n",
    "        headers = {'Accept': 'application/json'}\n",
    "        if type(volumes) == type(str()):\n",
    "            volumes = [volumes]\n",
    "        \n",
    "        for index, volume in enumerate(volumes):\n",
    "            url = baseURL + str(volume) + endURL\n",
    "\n",
    "            resp = requests.get(url, params=headers)\n",
    "            if not resp.ok:\n",
    "                raise ValueError('Response error; could not make call')\n",
    "            book_info = resp.json()\n",
    "            new_book = {}\n",
    "\n",
    "            try:\n",
    "                page_count  =  int(book_info['volumeInfo']['pageCount'])\n",
    "            except (KeyError):\n",
    "                page_count = 100\n",
    "\n",
    "            try:\n",
    "                categories = self.__category_preprocessor(book_info['volumeInfo']['categories'])\n",
    "            except (KeyError):\n",
    "                categories = self.__category_preprocessor(['Fiction'])\n",
    "\n",
    "            try:\n",
    "                average_rating = float(book_info['volumeInfo']['averageRating'])\n",
    "            except (KeyError):\n",
    "                average_rating = float(3)\n",
    "\n",
    "            try:\n",
    "                ratings_count = int(book_info['volumeInfo']['ratingsCount'])\n",
    "            except (KeyError):\n",
    "                ratings_count = int(0)\n",
    "\n",
    "            '''\n",
    "            try:\n",
    "                maturity_rating = book_info['volumeInfo']['maturityRating']\n",
    "            except (KeyError):\n",
    "                maturity_rating = 'NOT_MATURE'\n",
    "            '''\n",
    "\n",
    "            if labels != []:\n",
    "                book_data.append({\n",
    "                    'user_rating'       : labels[index],\n",
    "                    'page_count'        : page_count,\n",
    "                    'categories'        : categories,\n",
    "                    'average_rating'    : average_rating,\n",
    "                    'ratings_count'     : ratings_count\n",
    "                    #'maturity_rating'   : maturity_rating\n",
    "                })\n",
    "            else:\n",
    "                book_data.append({\n",
    "                    'page_count'        : page_count,\n",
    "                    'categories'        : categories,\n",
    "                    'average_rating'    : average_rating,\n",
    "                    'ratings_count'     : ratings_count\n",
    "                    #'maturity_rating'   : maturity_rating\n",
    "                })\n",
    "\n",
    "        return book_data\n",
    "\n",
    "\n",
    "    def __category_preprocessor(self, categories):\n",
    "        if type(categories) != type(list()):\n",
    "            return categories\n",
    "        \n",
    "        cats = []\n",
    "        for category in categories:\n",
    "            cat_split = category.split('/')\n",
    "            for cat in cat_split:\n",
    "                cat = cat.strip().lower()\n",
    "                if cat not in cats:\n",
    "                    cats.append(cat)\n",
    "\n",
    "        return cats\n",
    "\n",
    "\n",
    "    def __find_categories(self, books):\n",
    "        index = 2\n",
    "        for book in books:\n",
    "            for category in book['categories']:\n",
    "                if category not in self.categories.keys():\n",
    "                    self.categories[category] = index\n",
    "                    index += 1\n",
    "\n",
    "\n",
    "    def __find_maturities(self, books):\n",
    "        index = 0\n",
    "        for book in books:\n",
    "            if book['maturity_rating'] not in self.maturities:\n",
    "                self.maturities[book['maturity_rating']] = index\n",
    "                index += 1\n",
    "\n",
    "\n",
    "    def __reformat(self, books):\n",
    "        new_books = []\n",
    "        for book in books:\n",
    "            new_book = []\n",
    "            for data in book:\n",
    "                if 'categories' == data:\n",
    "                    for category in self.categories:\n",
    "                        new_book.append(0)\n",
    "                elif 'maturity_rating' == data:\n",
    "                    for maturity in self.maturities:\n",
    "                        new_book.append(0)\n",
    "                else:\n",
    "                    new_book.append(book[data])\n",
    "            new_books.append(new_book)\n",
    "        for index, book in enumerate(new_books):\n",
    "            for category in self.categories:\n",
    "                if category in books[index]['categories']:\n",
    "                    new_books[index][self.categories[category]] = books[index]['categories'].count(category)\n",
    "            '''for maturities in self.maturities:\n",
    "                if maturity in books[index]['maturities']:\n",
    "                    new_books[index][len(maturities[maturity])\n",
    "            '''\n",
    "        return new_books\n",
    "\n",
    "    def __reformat_volume(self, book):\n",
    "        new_book=[]\n",
    "        for data in book:\n",
    "            if 'categories' == data:\n",
    "                for category in self.categories:\n",
    "                    new_book.append(0)\n",
    "            else:\n",
    "                new_book.append(book[data])\n",
    "        for category in self.categories:\n",
    "            if category in book['categories']:\n",
    "                new_book[self.categories[category]] = book['categories'].count(category)\n",
    "        return new_book\n",
    "            \n",
    "\n",
    "    def __x_y_train(self, books):\n",
    "        books = deepcopy(books)\n",
    "        labels = []\n",
    "        new_books = []\n",
    "\n",
    "        for book in books:\n",
    "            item = deepcopy(book)\n",
    "            #label = np.ndarray(1)\n",
    "            #label[0] = int(item.pop(0))\n",
    "            #labels.append(label)\n",
    "            labels.append(int(item.pop(0)))\n",
    "            new_books.append(item)\n",
    "\n",
    "        return (np.array(new_books), np.array(labels))\n",
    "\n",
    "\n",
    "    def fit(self):\n",
    "        '''\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(keras.layers.Dense(units=20, input_dim=self.X_train.shape[1],\n",
    "                                     kernel_initializer='glorot_uniform',\n",
    "                                     bias_initializer='zeros',\n",
    "                                     activation='relu'))\n",
    "\n",
    "        model.add(keras.layers.Dense(units=20, input_dim=self.X_train.shape[1],\n",
    "                                     kernel_initializer='glorot_uniform',\n",
    "                                     bias_initializer='zeros',\n",
    "                                     activation='relu'))\n",
    "\n",
    "        model.add(keras.layers.Dense(units=self.y_train.shape[1], input_dim=self.X_train.shape[1],\n",
    "                                     kernel_initializer='glorot_uniform',\n",
    "                                     bias_initializer='zeros',\n",
    "                                     activation='softmax'))\n",
    "\n",
    "        sgd_optimizer = keras.optimizers.SGD(lr=0.001, decay=1e-7, momentum=0.9)\n",
    "\n",
    "        model.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "        history = model.fit(self.X_train, self.y_train,\n",
    "                            batch_size=3, epochs=15, verbose=1,\n",
    "                            validation_split=0.1)\n",
    "\n",
    "        self.y_train_pred = model.predict_classes(self.X_train, verbose=0)\n",
    "\n",
    "        '''\n",
    "        \n",
    "        pipe_lr = make_pipeline(StandardScaler(),\n",
    "                                PCA(),\n",
    "                                LogisticRegression(random_state=1, solver='lbfgs'))\n",
    "        \n",
    "        #self.y_train = \n",
    "        pipe_lr.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        self.y_train_pred = pipe_lr.predict(self.X_train)\n",
    "        \n",
    "        self.__model = pipe_lr\n",
    "        \n",
    "        #pipe_sgd = make_pipeline(StandardScaler(), PCA(), SGDClassifier(max_iter=1000, tol=1e-3))\n",
    "        #pipe_sgd.fit(self.X_train, self.y_train)\n",
    "        #self.y_train_pred = pipe_sgd.predict(self.X_train)\n",
    "        self.__model = pipe_sgd\n",
    "    \n",
    "        \n",
    "    def feature_space(self):\n",
    "        return str(self.X_train.shape[0]) + ', ' + str(self.X_train.shape[1])\n",
    "    \n",
    "    \n",
    "    def train_acc(self):\n",
    "        return np.sum(self.y_train_pred == self.y_train, axis=0) / self.y_train.shape[0]\n",
    "    \n",
    "        \n",
    "    def __preprocess_book(self, book):\n",
    "        new_book = deepcopy(book)\n",
    "        book_data = self.__pull_books(new_book)\n",
    "        \n",
    "        if type(book_data[0]) == type(dict()):\n",
    "            return np.asarray(self.__reformat_volume(book_data[0]))\n",
    "        raise TypeError(\"Goddamn\")\n",
    "    \n",
    "    def sample_data(self):\n",
    "        return 'Original:\\n' + str(self.__book_data[0]) + '\\nPost-processed:\\n' + str(self.X_train)\n",
    "    \n",
    "    \n",
    "    def predict(self, book=[]):\n",
    "        if book == [] or book ==\"\":\n",
    "            raise ValueError('Parameter cannot be an empty book')\n",
    "        if self.__model == None:\n",
    "            raise ValueError('Classifier needs training before predictions')\n",
    "        if type(book) != type(str()):\n",
    "            raise ValueError('Book must a valid volumeID of type \"String\"')\n",
    "\n",
    "        X_sample = self.__preprocess_book(book)\n",
    "        \n",
    "        return (self.__model.predict([X_sample]), self.__model.predict_proba([X_sample]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of volumes:\t 61\n",
      "Length of labels:\t 61\n"
     ]
    }
   ],
   "source": [
    "test_volumes = []\n",
    "test_labels = []\n",
    "\n",
    "with open('emily_books_2_tier.txt', 'r') as test_file:\n",
    "    for line in test_file:\n",
    "        items = line.split(',')\n",
    "        test_volumes.append(items[0])\n",
    "        test_labels.append(int(items[1].strip()))\n",
    "\n",
    "print('Length of volumes:\\t', len(test_volumes))\n",
    "print('Length of labels:\\t', len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Response error; could not make call",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-2196a9d3c7ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBookClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_volumes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-164-4c8bcd11e405>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, volumes, ratings)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-164-4c8bcd11e405>\u001b[0m in \u001b[0;36mset_up\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__book_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pull_books\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvolumes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__find_categories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__book_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m#self.maturities = self.__find_maturities(self.book_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-164-4c8bcd11e405>\u001b[0m in \u001b[0;36m__pull_books\u001b[0;34m(self, volumes, labels)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Response error; could not make call'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mbook_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mnew_book\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Response error; could not make call"
     ]
    }
   ],
   "source": [
    "test_model = BookClassifier(test_volumes, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_model = deepcopy(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories {'fiction': 2, 'thrillers': 3, 'suspense': 4, 'fantasy': 5, 'contemporary': 6, 'psychological': 7, 'young adult fiction': 8, 'science fiction': 9, 'space opera': 10, 'romance': 11, 'general': 12, 'occult & supernatural': 13, 'historical': 14, 'paranormal': 15, 'fairy tales, folk tales, legends & mythology': 16, 'action & adventure': 17, 'juvenile fiction': 18, 'wizards & witches': 19, 'school & education': 20, 'boarding school & prep school': 21, 'fantasy & magic': 22, 'history': 23, 'united states': 24, 'nature': 25, 'animals': 26, 'horses': 27, 'horror': 28, 'superheroes': 29, 'gaslamp': 30, 'literary': 31, 'family life': 32, 'coming of age': 33, 'epic': 34, 'dark fantasy': 35, 'classics': 36, 'media tie-in': 37, 'mystery & detective': 38, 'humorous': 39, 'absurdist': 40, 'dragons & mythical creatures': 41, 'romantic comedy': 42, 'women': 43, 'frankenstein (fictitious character)': 44, \"frankenstein's monster (fictitious character)\": 45, 'drama': 46, 'frankenstein, victor (fictitious character)': 47, 'horror plays': 48, 'monsters': 49, 'scientists': 50, 'american': 51, 'european': 52, 'english, irish, scottish, welsh': 53, 'magical realism': 54, 'psychology': 55, 'psychopathology': 56, 'social science': 57, 'popular culture': 58, 'vampires': 59, 'social themes': 60, 'dating & sex': 61, 'paranormal, occult & supernatural': 62, 'legends, myths, fables': 63, 'love & romance': 64, 'friendship': 65, 'dating & relationships': 66, 'poetry': 67, 'ancient & classical': 68, 'dystopian': 69, 'values & virtues': 70, 'survival stories': 71, 'self-esteem & self-reliance': 72, 'time travel': 73, 'boys & men': 74, 'erotica': 75, 'death & dying': 76, 'drugs, alcohol, substance abuse': 77, 'lifestyles': 78, 'city & town life': 79, 'new experience': 80}\n"
     ]
    }
   ],
   "source": [
    "print('Categories', backup_model.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "{'user_rating': 1, 'page_count': 352, 'categories': ['fiction', 'thrillers', 'suspense', 'fantasy', 'contemporary', 'psychological'], 'average_rating': 4.0, 'ratings_count': 34}\n",
      "Post-processed:\n",
      "[[3.520e+02 1.000e+00 1.000e+00 ... 0.000e+00 4.000e+00 3.400e+01]\n",
      " [6.080e+02 0.000e+00 0.000e+00 ... 0.000e+00 4.500e+00 5.700e+01]\n",
      " [5.920e+02 1.000e+00 1.000e+00 ... 0.000e+00 3.500e+00 6.790e+02]\n",
      " ...\n",
      " [4.850e+02 0.000e+00 0.000e+00 ... 1.000e+00 4.000e+00 2.473e+03]\n",
      " [3.840e+02 1.000e+00 1.000e+00 ... 0.000e+00 4.000e+00 1.540e+02]\n",
      " [3.080e+02 1.000e+00 0.000e+00 ... 0.000e+00 3.000e+00 0.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(backup_model.sample_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 96.72%\n"
     ]
    }
   ],
   "source": [
    "print('Train Accuracy: %.2f%%' % (100*backup_model.train_acc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "probability estimates are not available for loss='hinge'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-d393bb74b9eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mval_volume\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'bP60vwEACAAJ'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbackup_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_volume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-152-490893f5e1a6>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, book)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mX_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__preprocess_book\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_sample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_sample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, type)\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                     \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattribute_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \u001b[0mhttp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mjmlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsail\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medu\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mpapers\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mvolume2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mzhang02c\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mzhang02c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \"\"\"\n\u001b[0;32m-> 1032\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_proba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36m_check_proba\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"log\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"modified_huber\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             raise AttributeError(\"probability estimates are not available for\"\n\u001b[0;32m--> 994\u001b[0;31m                                  \" loss=%r\" % self.loss)\n\u001b[0m\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: probability estimates are not available for loss='hinge'"
     ]
    }
   ],
   "source": [
    "val_volume = 'bP60vwEACAAJ'\n",
    "backup_model.predict(val_volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factory method for classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(user_ratings=[], user_volumes=[]):\n",
    "    if user_ratings == [] or user_volumes == []:\n",
    "        raise TypeError('User ratings')\n",
    "    model = BookClassifier(user_ratings, user_volumes)\n",
    "    model.fit()\n",
    "    return deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mid_volume = 'yYPnwMpmeT4C'\n",
    "backup_model.predict(mid_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_volume = 'VO8nDwAAQBAJ'\n",
    "backup_model.predict(neg_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_volume = 'UbfnTcmkaKkC'\n",
    "backup_model.predict(neg_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_volume = 'rv5wCgAAQBAJ'\n",
    "backup_model.predict(neg_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_volume = 'Vuw7DwAAQBAJ'\n",
    "backup_model.predict(bad_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_volume = '5v1NBhR1W88C'\n",
    "backup_model.predict(bad_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_volume = 'kqgxDwAAQBAJ'\n",
    "backup_model.predict(bad_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
